把SievingNet放進一個DNN内，並且做訓練，紀錄省下計算的次數(這邊有兩種，一種是因為權重此位元為0所以省下來的，另一種是因為輸入exponent太小被過濾下來的，理論上這一部分才是我們的功勞)，和原本總計算次數相除，就能得知省下計算的次數。

把SievingNet做成硬體，一次做兩個向量內積，向量長度隨便，使用stream一次傳入兩個node(一個A的一個W的)還有目前篩檢閥值大小，SievingNet硬體做相乘。看輸出是否正確，並且查看硬體使用多寡。

探討: 
一般GPU如何設計
Strassen 演算法──分治矩陣乘法
稀疏矩陣


下次實驗: 
* 把"Siveve_layer_numpy_硬體邏輯版本.py"完成(完成)
* 測試im與r參數對結果準確率的影響
* 放進簡單神經網路之中宗柱寫的()，辨識3x5數字。
* 省下計算的次數。
* 放進進階神經網路之中NaiveSequential(786x512x10)，辨識MNIST。

