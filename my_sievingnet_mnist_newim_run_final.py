# -*- coding: utf-8 -*-
"""My_SievingNet_MNIST_newim_run.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H_gMQTEb5VVi0O_HoAfDzYmcTNRnRz2J
"""

import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
import matplotlib.pyplot as plt
import math

def count_elements(arr):
    unique_elements, counts = np.unique(arr, return_counts=True)
    result_dict = dict(zip(unique_elements, counts))
    return result_dict

def plot_exponent_counts(count_dict):
    keys = list(count_dict.keys())
    values = list(count_dict.values())

    plt.bar(keys, values)
    plt.xlabel('Exponent value')
    plt.ylabel('Count')
    plt.title('Count of Each Number')
    plt.show()


def float_to_fixed_point_binary(arr, int_bits=4, frac_bits=28):
    # 計算轉換係數
    scale = 2 ** frac_bits

    # 將浮點數乘以係數
    scaled_arr = np.round(arr * scale).astype(np.int64)

    # 計算整數的最大值和最小值
    max_val = (2 ** (int_bits + frac_bits)) - 1
    min_val = -(2 ** (int_bits + frac_bits))

    # 確保數值在允許的範圍內
    scaled_arr = np.clip(scaled_arr, min_val, max_val)

    def to_binary(val):
        if val >= 0:
            # 對於正數，直接轉成binary
            return np.unpackbits(np.array([val], dtype='>u4').view(np.uint8))[-(int_bits + frac_bits):]
            # >：Big-endian，u：Unsigned，2：two words（16bits），4: four words（32bits）
        else:
            # 對於正數，轉成binary最後再加上 - 號
            val = -val  # 先取絕對值
            bits = np.unpackbits(np.array([val], dtype='>u4').view(np.uint8))[-(int_bits + frac_bits):]
            return -bits

    # 將每個元素轉換為二進制
    binary_matrix = np.array([to_binary(val) for val in scaled_arr.flatten()])

    # 重塑為原始形狀並增加額外的維度
    result_shape = arr.shape + (int_bits + frac_bits,)
    binary_matrix = binary_matrix.reshape(result_shape)

    return binary_matrix.astype(np.int8)

def exponent(x):
    if x.dtype == np.float32:
        packed_data = x.view(np.uint32) # 先當作無號數看待，右移才會是unsigned extend數值才不會錯
        exponent_mask = 0x7F800000
        exponent_part_bias = (packed_data & exponent_mask) >> 23
        exponent_part = exponent_part_bias.view(np.int32) - 127 # 最後再看待乘有號數，才會有負數
    elif x.dtype == np.float64:
        packed_data = x.view(np.uint64)
        exponent_mask = 0x7FF0000000000000
        exponent_part_bias = (packed_data & exponent_mask) >> 52
        exponent_part = exponent_part_bias.view(np.int64) - 1023
    else:
        raise ValueError("Only float32 and float64 are supported.")

    return exponent_part

class SieveingNet():
    def __init__(self, samples, n_n):
        self.S_privious = np.zeros((samples, n_n), dtype=np.float32)

    def sieve(self, A, W, frac_bits, im):
        A = np.array(A, dtype=np.float32)
        W = np.array(W, dtype=np.int8)
        frac_bits = np.int32(frac_bits)
        im = np.float32(im)

        n_l, n_n, B = W.shape
        samples = A.shape[0]

        # Initialize X with zeros
        X = np.zeros((samples, n_n), dtype=np.float32)

        # Pre-compute bit orders and powers
        bit_orders = np.arange(B-1, -1, -1, dtype=np.float32)  # shape: (B,)
        bit_orders_expanded = bit_orders[np.newaxis, np.newaxis, np.newaxis, :] # shape: (1, 1, 1, B)
        powers = bit_orders - frac_bits  # shape: (B,)

        # Expand dimensions for broadcasting
        A_expanded = A[:, :, np.newaxis, np.newaxis]  # shape: (samples, n_l, 1, 1)
        A_expanded = np.tile(A_expanded, (1, 1, n_n, B))  # shape: (samples, n_l, n_n, B)
        W_expanded = W[np.newaxis, :, :, :]  # shape: (1, n_l, n_n, B)

        # Compute exponents of A
        exponents_A = exponent(A)[:, :, np.newaxis, np.newaxis]  # shape: (samples, n_l, 1, 1)
        exponents_S_p = exponent(self.S_privious)[:, np.newaxis, :]  # shape: (samples, 1, n_n)
        exponents_S_p_expanded = exponents_S_p[:, :, :, np.newaxis] # shape: (samples, 1, n_n, 1)
        # Compute sieving thresholds for all bits
        sieving_thresholds = exponents_S_p_expanded - im * (exponents_S_p_expanded - (-127)) + 32*(1 - im) - bit_orders_expanded# shape: (samples, 1, n_n, B)
        # Create mask where W is non-zero for each bit
        mask = W_expanded.astype(np.float32)  # shape: (1, n_l, n_n, B)
        mask = np.tile(mask, (samples, 1, 1, 1))  # shape: (samples, n_l, n_n, B)
        input_sieve_mask = np.where(
            exponents_A >= sieving_thresholds,
            1.0,
            0.0
        )  # shape: (samples, n_l, n_n, B)

        # Calculate contributions for each bit
        contributions = np.where(
            exponents_A >= sieving_thresholds,
            A_expanded * mask,
            0.0
        )  # shape: (samples, n_l, n_n, B)

        # Sum contributions across input nodes and bits
        x_k = np.sum(contributions, axis=1)  # shape: (samples, n_n, B)

        # Calculate final X
        X = np.sum(x_k * (2.0 ** powers), axis=2)  # shape: (samples, n_n)


        mask_non_zero_count = np.count_nonzero(mask)
        input_non_zero_count = samples * n_l * n_n * B
        input_sieve_non_zero_count = np.count_nonzero(input_sieve_mask)

        # Update S_privious
        self.S_privious = X
        return X, input_non_zero_count, input_sieve_non_zero_count

import tensorflow as tf
from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt
import math

# 載入MNIST資料集
(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()

# 將圖片數據展平成一維向量
X_train = X_train.reshape(-1, 28 * 28).astype('float32')
X_test = X_test.reshape(-1, 28 * 28).astype('float32')

# 將數據標準化到0-1之間
X_train /= 255
X_test /= 255

# 將標籤進行One-hot編碼
y_train = tf.keras.utils.to_categorical(y_train, 10)
y_test = tf.keras.utils.to_categorical(y_test, 10)

# 如果需要再一次進行訓練集和測試集的分割
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

X_train = X_train[:128*48, ]
y_train = y_train[:128*48, ]
X_val = X_val[:128*8, ]
y_val = y_val[:128*8, ]
X_test = X_test[:128*8, ]
y_test = y_test[:128*8, ]
print("訓練集大小:", X_train.shape, y_train.shape)
print("驗證集大小:", X_val.shape, y_val.shape)
print("測試集大小:", X_test.shape, y_test.shape)

"""### 使用了RMSprop+adaptive learning rate 方法"""

# 初始化數據和權重
input_size = X_train.shape[1]
hidden_size = 128
output_size = y_train.shape[1]

# 激活函數和其導數
def softmax(x):
    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
    return exp_x / np.sum(exp_x, axis=1, keepdims=True)

def relu(x):
    return np.maximum(0, x)

def relu_derivative(x):
    return np.where(x > 0, 1, 0)

# 學習前向傳播
def forward_propagation(X, W1, b1, W2, b2, W1_fixed, W2_fixed, im):
    frac_bits = 28
    befor_count = 0
    after_count = 0
    Z1, before_temp, after_temp = SieveingNet1.sieve(X, W1_fixed, frac_bits=frac_bits, im=im)
    Z1 = Z1 + b1
    befor_count += before_temp
    after_count += after_temp
    A1 = relu(Z1)
    Z2, before_temp, after_temp = SieveingNet2.sieve(A1, W2_fixed, frac_bits=frac_bits, im=im)
    Z2 = Z2 + b2
    befor_count += before_temp
    after_count += after_temp
    A2 = softmax(Z2)
    return Z1, A1, Z2, A2 , befor_count, after_count

# 推論前向傳播
def forward_propagation_inference(X, W1, b1, W2, b2):
    Z1 = np.dot(X, W1) + b1
    A1 = relu(Z1)
    Z2 = np.dot(A1, W2) + b2
    A2 = softmax(Z2)
    return Z1, A1, Z2, A2

# 損失計算
def compute_loss(y_true, y_pred):
    epsilon = 1e-10
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    return -np.sum(y_true * np.log(y_pred)) / y_true.shape[0]

# 反向傳播
def backward_propagation(X, Y, Z1, A1, Z2, A2, W2):
    m = X.shape[0]

    # Output layer gradients
    dZ2 = A2 - Y
    dW2 = np.dot(A1.T, dZ2) / m
    db2 = np.sum(dZ2, axis=0, keepdims=True) / m

    # Hidden layer gradients
    dA1 = np.dot(dZ2, W2.T)
    dZ1 = dA1 * relu_derivative(Z1)
    dW1 = np.dot(X.T, dZ1) / m
    db1 = np.sum(dZ1, axis=0, keepdims=True) / m

    return dW1, db1, dW2, db2

# 在程式碼中加入紀錄和繪圖部分
epochs_list = []
before_counts_list = []
after_counts_list = []
accuracy_list = []

# 設定超參數
learning_rate = 0.001
total_epochs = 10
batch_size = 128
decay_rate = 0.9  # RMSprop 衰減率

# 初始化權重和偏差
# np.random.seed(12)
np.random.seed(14)
W1 = np.random.randn(input_size, hidden_size).astype(np.float32) * np.sqrt(2 / input_size)
b1 = np.zeros((1, hidden_size), dtype=np.float32)
W2 = np.random.randn(hidden_size, output_size).astype(np.float32) * np.sqrt(2 / hidden_size)
b2 = np.zeros((1, output_size), dtype=np.float32)

# 初始化 RMSprop 累積梯度平方
s_W1 = np.zeros_like(W1)
s_b1 = np.zeros_like(b1)
s_W2 = np.zeros_like(W2)
s_b2 = np.zeros_like(b2)

# 轉換權重到固定點二進制表示
W1_fixed = float_to_fixed_point_binary(W1, int_bits=4, frac_bits=28)
W2_fixed = float_to_fixed_point_binary(W2, int_bits=4, frac_bits=28)

# 定義 SievingNet 類別
SieveingNet1 = SieveingNet(batch_size, hidden_size)
SieveingNet2 = SieveingNet(batch_size, output_size)

# 儲存各種數據list
save_rate_history = []
train_loss_history = []
val_loss_history = []
accuracy_val_history = []
test_loss_history = []
im_history_t = []
accuracy_test_history = []

befor_count = 0
after_count = 0

imStart = 0.1
im = imStart
imEnd = 1
k = 5

num_samples = X_train.shape[0]

# 訓練過程
for current_epoch in range(total_epochs):
    train_epoch_loss = 0
    # 打亂訓練數據
    permutation = np.random.permutation(num_samples)
    X_train_shuffled = X_train[permutation]
    y_train_shuffled = y_train[permutation]
    print(f"Epoch {current_epoch + 1}/{total_epochs}, im: {im:4f}")
    # 逐批次訓練
    for i in range(0, num_samples, batch_size):
        X_batch = X_train_shuffled[i:i + batch_size]
        y_batch = y_train_shuffled[i:i + batch_size]
        # 前向傳播
        Z1, A1, Z2, A2, before_temp, after_temp = forward_propagation(X_batch, W1, b1, W2, b2, W1_fixed, W2_fixed, im)
        befor_count += before_temp
        after_count += after_temp
        loss = compute_loss(y_batch, A2)
        train_epoch_loss += loss

        # 反向傳播
        dW1, db1, dW2, db2 = backward_propagation(X_batch, y_batch, Z1, A1, Z2, A2, W2)

        # 應用 RMSprop
        s_W1 = decay_rate * s_W1 + (1 - decay_rate) * np.square(dW1)
        s_b1 = decay_rate * s_b1 + (1 - decay_rate) * np.square(db1)
        s_W2 = decay_rate * s_W2 + (1 - decay_rate) * np.square(dW2)
        s_b2 = decay_rate * s_b2 + (1 - decay_rate) * np.square(db2)

        W1 -= learning_rate * dW1 / (np.sqrt(s_W1) + 1e-8)
        b1 -= learning_rate * db1 / (np.sqrt(s_b1) + 1e-8)
        W2 -= learning_rate * dW2 / (np.sqrt(s_W2) + 1e-8)
        b2 -= learning_rate * db2 / (np.sqrt(s_b2) + 1e-8)

        # 轉換權重到固定點二進制表示
        W1_fixed = float_to_fixed_point_binary(W1, int_bits=4, frac_bits=28)
        W2_fixed = float_to_fixed_point_binary(W2, int_bits=4, frac_bits=28)

        # 評估測試集準確率-用來和計算成本作圖的
        _, _, _, A2_test = forward_propagation_inference(X_test, W1, b1, W2, b2)
        predictions = np.argmax(A2_test, axis=1)
        labels = np.argmax(y_test, axis=1)
        accuracy_test = np.mean(predictions == labels)
        accuracy_test_history.append(accuracy_test)

        before_counts_list.append(befor_count)
        after_counts_list.append(after_count)
    # 每個epoch的save rate
    if 1:
      save_temp = before_temp - after_temp
      save_rate_epoch = save_temp / before_temp
      save_rate_history.append(save_rate_epoch)
      print(f"the number of additions(old): {befor_count:.3e}, the number of additions(new): {after_count:.3e}, save_rate_epoch: {save_rate_epoch * 100:.3f}%")
    # 每個 epoch 結束時記錄一次訓練損失
    train_epoch_loss /= (X_train.shape[0] // batch_size)
    # 自適應學習率
    if current_epoch > 0:
        if train_epoch_loss < train_loss_history[-1]:
            learning_rate *= 0.95  # 如果損失減少，稍微減少學習率
        else:
            learning_rate *= 1.05  # 如果損失增加，增加學習率
    train_loss_history.append(train_epoch_loss)
    # 評估驗証集準確率
    _, _, _, A2_val = forward_propagation_inference(X_val, W1, b1, W2, b2)
    val_loss = compute_loss(y_val, A2_val)
    val_loss_history.append(val_loss)
    predictions = np.argmax(A2_val, axis=1)
    labels = np.argmax(y_val, axis=1)
    accuracy_val = np.mean(predictions == labels)
    accuracy_val_history.append(accuracy_val)

    # 評估測試集損失
    _, _, _, A2_test = forward_propagation_inference(X_test, W1, b1, W2, b2)
    test_loss = compute_loss(y_test, A2_test)
    test_loss_history.append(test_loss)
    print(f"Training loss: {train_epoch_loss:.4f}, Validation accuracy: {accuracy_val:.3f}, Test loss: {test_loss:.4f}, Learning rate: {learning_rate:.6f}")

    # 調整im超參數
    im_history_t.append(im)
    if current_epoch+1 < total_epochs - 1:
      # im = imEnd - (imEnd - imStart) * np.exp(-k * current_epoch+1 / total_epochs-1)
      # im = imEnd - (imEnd - imStart) * np.exp(-k * accuracy_val / 1)
      im = imEnd - (imEnd - imStart) * np.exp(-k * (current_epoch+1) / total_epochs)
    else:
      im = 1
    print("------------------------------")
    epochs_list.append(current_epoch)

_, _, _, A2_test = forward_propagation_inference(X_test, W1, b1, W2, b2)
predictions = np.argmax(A2_test, axis=1)
labels = np.argmax(y_test, axis=1)
accuracy_test = np.mean(predictions == labels)
print(f"Final accuracy(on test): {accuracy_test:.4f}")

"""### 冪次方方法"""

# 初始化數據和權重
input_size = X_train.shape[1]
hidden_size = 128
output_size = y_train.shape[1]

# 激活函數和其導數
def softmax(x):
    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
    return exp_x / np.sum(exp_x, axis=1, keepdims=True)

def relu(x):
    return np.maximum(0, x)

def relu_derivative(x):
    return np.where(x > 0, 1, 0)

# 學習前向傳播
def forward_propagation(X, W1, b1, W2, b2, W1_fixed, W2_fixed, im):
    frac_bits = 28
    befor_count = 0
    after_count = 0
    Z1, before_temp, after_temp = SieveingNet1.sieve(X, W1_fixed, frac_bits=frac_bits, im=im)
    Z1 = Z1 + b1
    befor_count += before_temp
    after_count += after_temp
    A1 = relu(Z1)
    Z2, before_temp, after_temp = SieveingNet2.sieve(A1, W2_fixed, frac_bits=frac_bits, im=im)
    Z2 = Z2 + b2
    befor_count += before_temp
    after_count += after_temp
    A2 = softmax(Z2)
    return Z1, A1, Z2, A2 , befor_count, after_count

# 推論前向傳播
def forward_propagation_inference(X, W1, b1, W2, b2):
    Z1 = np.dot(X, W1) + b1
    A1 = relu(Z1)
    Z2 = np.dot(A1, W2) + b2
    A2 = softmax(Z2)
    return Z1, A1, Z2, A2

# 損失計算
def compute_loss(y_true, y_pred):
    epsilon = 1e-10
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    return -np.sum(y_true * np.log(y_pred)) / y_true.shape[0]

# 反向傳播
def backward_propagation(X, Y, Z1, A1, Z2, A2, W2):
    m = X.shape[0]

    # Output layer gradients
    dZ2 = A2 - Y
    dW2 = np.dot(A1.T, dZ2) / m
    db2 = np.sum(dZ2, axis=0, keepdims=True) / m

    # Hidden layer gradients
    dA1 = np.dot(dZ2, W2.T)
    dZ1 = dA1 * relu_derivative(Z1)
    dW1 = np.dot(X.T, dZ1) / m
    db1 = np.sum(dZ1, axis=0, keepdims=True) / m

    return dW1, db1, dW2, db2

# 在程式碼中加入紀錄和繪圖部分
epochs_list = []
before_counts_list = []
after_counts_list = []
accuracy_list = []

# 設定超參數
learning_rate = 0.001
total_epochs = 10
batch_size = 128
decay_rate = 0.9  # RMSprop 衰減率

# 初始化權重和偏差
np.random.seed(12)
W1 = np.random.randn(input_size, hidden_size).astype(np.float32) * np.sqrt(2 / input_size)
b1 = np.zeros((1, hidden_size), dtype=np.float32)
W2 = np.random.randn(hidden_size, output_size).astype(np.float32) * np.sqrt(2 / hidden_size)
b2 = np.zeros((1, output_size), dtype=np.float32)

# 初始化 RMSprop 累積梯度平方
s_W1 = np.zeros_like(W1)
s_b1 = np.zeros_like(b1)
s_W2 = np.zeros_like(W2)
s_b2 = np.zeros_like(b2)

# 轉換權重到固定點二進制表示
W1_fixed = float_to_fixed_point_binary(W1, int_bits=4, frac_bits=28)
W2_fixed = float_to_fixed_point_binary(W2, int_bits=4, frac_bits=28)

# 定義 SievingNet 類別
SieveingNet1 = SieveingNet(batch_size, hidden_size)
SieveingNet2 = SieveingNet(batch_size, output_size)

# 儲存各種數據list
save_rate_history = []
train_loss_history = []
val_loss_history = []
accuracy_val_history = []
test_loss_history = []
im_history_t = []
accuracy_test_history = []

befor_count = 0
after_count = 0

imStart = 0.01
im = imStart
imEnd = 1
p = 4

num_samples = X_train.shape[0]

# 訓練過程
for current_epoch in range(total_epochs):
    train_epoch_loss = 0
    # 打亂訓練數據
    permutation = np.random.permutation(num_samples)
    X_train_shuffled = X_train[permutation]
    y_train_shuffled = y_train[permutation]
    print(f"Epoch {current_epoch + 1}/{total_epochs}, im: {im:4f}")
    # 逐批次訓練
    for i in range(0, num_samples, batch_size):
        X_batch = X_train_shuffled[i:i + batch_size]
        y_batch = y_train_shuffled[i:i + batch_size]
        # 前向傳播
        Z1, A1, Z2, A2, before_temp, after_temp = forward_propagation(X_batch, W1, b1, W2, b2, W1_fixed, W2_fixed, im)
        befor_count += before_temp
        after_count += after_temp
        loss = compute_loss(y_batch, A2)
        train_epoch_loss += loss

        # 反向傳播
        dW1, db1, dW2, db2 = backward_propagation(X_batch, y_batch, Z1, A1, Z2, A2, W2)

        # 應用 RMSprop
        s_W1 = decay_rate * s_W1 + (1 - decay_rate) * np.square(dW1)
        s_b1 = decay_rate * s_b1 + (1 - decay_rate) * np.square(db1)
        s_W2 = decay_rate * s_W2 + (1 - decay_rate) * np.square(dW2)
        s_b2 = decay_rate * s_b2 + (1 - decay_rate) * np.square(db2)

        W1 -= learning_rate * dW1 / (np.sqrt(s_W1) + 1e-8)
        b1 -= learning_rate * db1 / (np.sqrt(s_b1) + 1e-8)
        W2 -= learning_rate * dW2 / (np.sqrt(s_W2) + 1e-8)
        b2 -= learning_rate * db2 / (np.sqrt(s_b2) + 1e-8)

        # 轉換權重到固定點二進制表示
        W1_fixed = float_to_fixed_point_binary(W1, int_bits=4, frac_bits=28)
        W2_fixed = float_to_fixed_point_binary(W2, int_bits=4, frac_bits=28)

        # 評估測試集準確率-用來和計算成本作圖的
        _, _, _, A2_test = forward_propagation_inference(X_test, W1, b1, W2, b2)
        predictions = np.argmax(A2_test, axis=1)
        labels = np.argmax(y_test, axis=1)
        accuracy_test = np.mean(predictions == labels)
        accuracy_test_history.append(accuracy_test)

        before_counts_list.append(befor_count)
        after_counts_list.append(after_count)
    # 每個epoch的save rate
    if 1:
      save_temp = before_temp - after_temp
      save_rate_epoch = save_temp / before_temp
      save_rate_history.append(save_rate_epoch)
      print(f"save_rate_epoch: {save_rate_epoch * 100:.3f}%")
    # 每個 epoch 結束時記錄一次訓練損失
    train_epoch_loss /= (X_train.shape[0] // batch_size)
    # 自適應學習率
    if current_epoch > 0:
        if train_epoch_loss < train_loss_history[-1]:
            learning_rate *= 0.95  # 如果損失減少，稍微減少學習率
        else:
            learning_rate *= 1.05  # 如果損失增加，增加學習率
    train_loss_history.append(train_epoch_loss)
    # 評估驗証集準確率
    _, _, _, A2_val = forward_propagation_inference(X_val, W1, b1, W2, b2)
    val_loss = compute_loss(y_val, A2_val)
    val_loss_history.append(val_loss)
    predictions = np.argmax(A2_val, axis=1)
    labels = np.argmax(y_val, axis=1)
    accuracy_val = np.mean(predictions == labels)
    accuracy_val_history.append(accuracy_val)

    # 評估測試集損失
    _, _, _, A2_test = forward_propagation_inference(X_test, W1, b1, W2, b2)
    test_loss = compute_loss(y_test, A2_test)
    test_loss_history.append(test_loss)
    print(f"Training loss: {train_epoch_loss:.4f}, Validation accuracy: {accuracy_val:.3f}, Test loss: {test_loss:.4f}, Learning rate: {learning_rate:.6f}")

    # 調整im超參數
    im_history_t.append(im)
    if current_epoch+1 < total_epochs - 1:
      # im = imEnd - (imEnd - imStart) * np.exp(-k * accuracy_val / 1)
      im = imEnd - (imEnd - imStart) * np.exp(-k * (current_epoch+1) / total_epochs)
      # im = imStart + (imEnd - imStart) * (accuracy_val / 1.0)**p
    else:
      im = 1
    print("------------------------------")
    epochs_list.append(current_epoch)

_, _, _, A2_test = forward_propagation_inference(X_test, W1, b1, W2, b2)
predictions = np.argmax(A2_test, axis=1)
labels = np.argmax(y_test, axis=1)
accuracy_test = np.mean(predictions == labels)
print(f"Final accuracy(on test): {accuracy_test:.4f}")

import matplotlib.pyplot as plt

plt.figure(figsize=(15, 10))

# 第一排
plt.subplot(2, 3, 1)
plt.plot(range(1, total_epochs + 1), train_loss_history)
plt.title('Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')

plt.subplot(2, 3, 2)
plt.plot(range(1, total_epochs + 1), accuracy_val_history)
plt.title('Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')

plt.subplot(2, 3, 3)
plt.plot(range(1, total_epochs + 1), test_loss_history)
plt.title('Test Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')

plt.subplot(2, 3, 4)
plt.plot(range(1, total_epochs + 1), val_loss_history)
plt.title('Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')

plt.subplot(2, 3, 5)
plt.plot(range(1, total_epochs + 1), val_loss_history)
plt.title('Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')

plt.subplot(2, 3, 6)
plt.plot(range(1, 481), accuracy_test_history)
plt.title('Test Accuracy')
plt.xlabel('Batch')
plt.ylabel('Accuracy')

plt.tight_layout()  # 自動調整子圖之間的間距
plt.show()

import matplotlib.pyplot as plt

# 創建一個圖形
plt.figure(figsize=(10, 5))

# 繪製 before_count 的數據點
plt.scatter(before_counts_list, accuracy_test_history, label='Conventional', color='blue', marker='o', s=15)

# 繪製 after_count 的數據點
plt.scatter(after_counts_list, accuracy_test_history, label='SievingNet', color='orange', marker='s', s=15)

# 添加標題和標籤
plt.title('Before Count and After Count over Accuracy')
plt.xlabel('Computing costs')
plt.ylabel('Test Accuracy')

# 添加圖例
plt.legend()

# 顯示圖形
plt.show()

save_count = befor_count - after_count
save_rate = save_count / befor_count
print(f"save_count: {save_count}")
print(f"save_rate: {save_rate * 100}%")

X_val = X_val[:128, ]
y_val = y_val[:128, ]

_, _, _, A2_val, _, _ = forward_propagation(X_val, W1, b1, W2, b2, W1_fixed, W2_fixed, im) # 耗時8秒
predictions = np.argmax(A2_val, axis=1)
labels = np.argmax(y_val, axis=1)
accuracy = np.mean(predictions == labels)

accuracy

X_test = X_test[:128, ]
y_test = y_test[:128, ]

_, _, _, A2_test, _, _ = forward_propagation(X_test, W1, b1, W2, b2, W1_fixed, W2_fixed, im)
test_loss = compute_loss(y_test, A2_test)

test_loss

import numpy as np
import math

# 初始化數據和權重（X_train, y_train, X_val, y_val, X_test, y_test 已經定義）
input_size = X_train.shape[1]
hidden_size = 128
output_size = y_train.shape[1]

# 激活函數和其導數
def softmax(x):
    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))
    return exp_x / np.sum(exp_x, axis=1, keepdims=True)

def relu(x):
    return np.maximum(0, x)

def relu_derivative(x):
    return np.where(x > 0, 1, 0)

# 前向傳播
def forward_propagation(X, W1, b1, W2, b2):
    Z1 = np.dot(X, W1) + b1
    A1 = relu(Z1)
    Z2 = np.dot(A1, W2) + b2
    A2 = softmax(Z2)
    return Z1, A1, Z2, A2

# 損失計算
def compute_loss(y_true, y_pred):
    epsilon = 1e-10
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    return -np.sum(y_true * np.log(y_pred)) / y_true.shape[0]

# 反向傳播
def backward_propagation(X, Y, Z1, A1, Z2, A2, W2):
    m = X.shape[0]

    # Output layer gradients
    dZ2 = A2 - Y
    dW2 = np.dot(A1.T, dZ2) / m
    db2 = np.sum(dZ2, axis=0, keepdims=True) / m

    # Hidden layer gradients
    dA1 = np.dot(dZ2, W2.T)
    dZ1 = dA1 * relu_derivative(Z1)
    dW1 = np.dot(X.T, dZ1) / m
    db1 = np.sum(dZ1, axis=0, keepdims=True) / m

    return dW1, db1, dW2, db2

# 設定超參數
learning_rate = 0.001  # 初始學習率
batch_size = 128
decay_rate = 0.9  # RMSprop 衰減率

np.random.seed(20)
# np.random.seed(18)
# np.random.seed(16)
# np.random.seed(14)
# np.random.seed(12)
W1 = np.random.randn(input_size, hidden_size).astype(np.float32) * np.sqrt(2 / input_size)
b1 = np.zeros((1, hidden_size), dtype=np.float32)
W2 = np.random.randn(hidden_size, output_size).astype(np.float32) * np.sqrt(2 / hidden_size)
b2 = np.zeros((1, output_size), dtype=np.float32)

# 初始化 RMSprop 累積梯度平方
s_W1 = np.zeros_like(W1)
s_b1 = np.zeros_like(b1)
s_W2 = np.zeros_like(W2)
s_b2 = np.zeros_like(b2)

# 儲存訓練損失和測試準確率
train_loss_history_t = [] # t mean traditional
val_loss_history_t = []
accuracy_val_history_t = []
test_loss_history_t = []
im_history_t = []
accuracy_test_history_t = []

e_last_t = 0
e_present_t = 0

imStart = 0.01
im = imStart
imEnd = 1
k = 5
total_epochs = 10

num_samples = X_train.shape[0]

# 訓練過程
for current_epoch in range(total_epochs):
    epoch_loss_t = 0
    # 打亂訓練數據
    permutation = np.random.permutation(num_samples)
    X_train_shuffled = X_train[permutation]
    y_train_shuffled = y_train[permutation]
    print(f"im: {im}")
    # 逐批次訓練
    for i in range(0, num_samples, batch_size):
        X_batch = X_train_shuffled[i:i + batch_size]
        y_batch = y_train_shuffled[i:i + batch_size]

        # 前向傳播
        Z1, A1, Z2, A2 = forward_propagation(X_batch, W1, b1, W2, b2)
        loss = compute_loss(y_batch, A2)
        epoch_loss_t += loss

        # 反向傳播
        dW1, db1, dW2, db2 = backward_propagation(X_batch, y_batch, Z1, A1, Z2, A2, W2)

        # 應用 RMSprop
        s_W1 = decay_rate * s_W1 + (1 - decay_rate) * np.square(dW1)
        s_b1 = decay_rate * s_b1 + (1 - decay_rate) * np.square(db1)
        s_W2 = decay_rate * s_W2 + (1 - decay_rate) * np.square(dW2)
        s_b2 = decay_rate * s_b2 + (1 - decay_rate) * np.square(db2)

        W1 -= learning_rate * dW1 / (np.sqrt(s_W1) + 1e-8)
        b1 -= learning_rate * db1 / (np.sqrt(s_b1) + 1e-8)
        W2 -= learning_rate * dW2 / (np.sqrt(s_W2) + 1e-8)
        b2 -= learning_rate * db2 / (np.sqrt(s_b2) + 1e-8)

        # 評估測試集準確率
        _, _, _, A2_test = forward_propagation(X_test, W1, b1, W2, b2)
        predictions = np.argmax(A2_test, axis=1)
        labels = np.argmax(y_test, axis=1)
        accuracy_test = np.mean(predictions == labels)
        accuracy_test_history_t.append(accuracy_test)

    # 每個 epoch 結束時顯示損失
    epoch_loss_t /= (num_samples // batch_size)

    # 自適應學習率
    if current_epoch > 0:
        if epoch_loss_t < train_loss_history_t[-1]:
            learning_rate *= 0.95  # 如果損失減少，稍微增加學習率
        else:
            learning_rate *= 1.05  # 如果損失增加，減少學習率

    # 每個 epoch 結束時記錄一次訓練損失和準確率和測試損失
    train_loss_history_t.append(epoch_loss_t)
    _, _, _, A2_val = forward_propagation(X_val, W1, b1, W2, b2)
    val_loss_t = compute_loss(y_val, A2_val)
    val_loss_history_t.append(val_loss_t)
    predictions = np.argmax(A2_val, axis=1)
    labels = np.argmax(y_val, axis=1)
    accuracy_val_t = np.mean(predictions == labels)
    accuracy_val_history_t.append(accuracy_val_t)

    _, _, _, A2 = forward_propagation(X_test, W1, b1, W2, b2)
    test_loss_t = compute_loss(y_test, A2)
    test_loss_history_t.append(test_loss_t)
    print(f"Epoch {current_epoch + 1}/{total_epochs}, Training loss: {epoch_loss_t:.4f}, Validation accuracy: {accuracy_val_t:.3f}, Test loss: {test_loss_t:.4f}, Learning rate: {learning_rate:.6f}")

    # 調整im超參數
    if current_epoch < total_epochs - 2:
      im = imEnd - (imEnd - imStart) * np.exp(-k * current_epoch / total_epochs-1)
      # im = imEnd - (imEnd - imStart) * np.exp(-k * accuracy_val_t / 1)
      # im = imStart + (imEnd - imStart) * (accuracy_val_t / 1.0)**p
    else:
      im = 1
    im_history_t.append(im)
    print("------------------------------")


_, _, _, A2_test = forward_propagation(X_test, W1, b1, W2, b2)
predictions = np.argmax(A2_test, axis=1)
labels = np.argmax(y_test, axis=1)
accuracy_test_t = np.mean(predictions == labels)
print(f"Final accuracy(on test): {accuracy_test_t:.4f}")

# 92.09
# 92.09
# 92.48
# 91.21
# 91.86
train_loss_history_t

import matplotlib.pyplot as plt

plt.figure(figsize=(15, 10))

# 第一排
plt.subplot(2, 3, 1)
plt.plot(range(1, total_epochs + 1), train_loss_history_t)
plt.title('Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')

plt.subplot(2, 3, 2)
plt.plot(range(1, total_epochs + 1), accuracy_val_history_t)
plt.title('Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')

plt.subplot(2, 3, 3)
plt.plot(range(1, total_epochs + 1), test_loss_history_t)
plt.title('Test Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')

# 第二排
plt.subplot(2, 3, 4)
plt.plot(range(1, total_epochs + 1), im_history_t)
plt.title('im')
plt.xlabel('Epoch')
plt.ylabel('Im')

plt.subplot(2, 3, 5)
plt.plot(range(1, total_epochs + 1), val_loss_history_t)
plt.title('Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')

plt.subplot(2, 3, 6)
plt.plot(range(1, 481), accuracy_test_history_t)
plt.title('Test Accuracy')
plt.xlabel('Batch')
plt.ylabel('Accuracy')

plt.tight_layout()  # 自動調整子圖之間的間距
plt.show()

# 創建一個圖形
plt.figure(figsize=(10, 5))

# 繪製 before_count 的數據線
plt.plot(epochs_list, train_loss_history_t, label='Conventional', color='blue', marker='o')

# 繪製 after_count 的數據線
plt.plot(epochs_list, train_loss_history, label='SievingNet', color='orange', marker='s')
# 添加標題和標籤
plt.title('Traning set loss vs Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
# 添加圖例
plt.legend()
# 顯示圖形
plt.show()

# 創建一個圖形
plt.figure(figsize=(10, 5))

# 繪製 before_count 的數據線
plt.plot(epochs_list, val_loss_history_t, label='Conventional', color='blue', marker='o')

# 繪製 after_count 的數據線
plt.plot(epochs_list, val_loss_history, label='SievingNet', color='orange', marker='s')
# 添加標題和標籤
plt.title('Validation set loss vs Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
# 添加圖例
plt.legend()
# 顯示圖形
plt.show()


# 創建一個圖形
plt.figure(figsize=(10, 5))

# 繪製 before_count 的數據線
plt.plot(epochs_list, accuracy_val_history_t, label='Conventional', color='blue', marker='o')

# 繪製 after_count 的數據線
plt.plot(epochs_list, accuracy_val_history, label='SievingNet', color='orange', marker='s')
# 添加標題和標籤
plt.title('Validation set accuracy vs Epochs')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
# 添加圖例
plt.legend()
# 顯示圖形
plt.show()

# 創建一個圖形
plt.figure(figsize=(10, 5))

# 繪製 before_count 的數據線
plt.plot(epochs_list, test_loss_history_t, label='Conventional', color='blue', marker='o')

# 繪製 after_count 的數據線
plt.plot(epochs_list, test_loss_history, label='SievingNet', color='orange', marker='s')
# 添加標題和標籤
plt.title('Testing set loss vs Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
# 添加圖例
plt.legend()
# 顯示圖形
plt.show()

# 創建一個圖形
plt.figure(figsize=(10, 5))

# 繪製 before_count 的數據線
plt.scatter(range(1,481,1), accuracy_test_history_t, label='Conventional', color='blue', marker='o', s=15)

# 繪製 after_count 的數據線
plt.scatter(range(1,481,1), accuracy_test_history, label='SievingNet', color='orange', marker='s', s=15)
# 添加標題和標籤
plt.title('Test Accuracy vs Batchs')
plt.xlabel('Batchs')
plt.ylabel('Accuracy')
# 添加圖例
plt.legend()
# 顯示圖形
plt.show()

